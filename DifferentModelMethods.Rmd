---
title: "DifferentModelMethod"
author: "Hoang Le Xuan"
date: "2024-04-06"
output: html_document
---

Reference:https://rspatial.org/

## Model Methods
```{r}
library(dismo)
library(maptools)

library(raster)
library(sp)

#Uncomment if you wanna downlaod these packages
#install.packages("e1071")
#install.packages('caret', dependencies = TRUE)
library(e1071)


data(wrld_simpl)
predictors <- stack(list.files(path="data/bioclim",  pattern='asc',  full.names=TRUE ))

canetoad <- read.table("data/ctsel.csv", header = T, sep=',')
canetoad <- canetoad[,-1]
presvals <- extract(predictors, canetoad)

set.seed(0)
backgr <- randomPoints(predictors, 500)
absvals <- extract(predictors, backgr)
pb <- c(rep(1, nrow(presvals)), rep(0, nrow(absvals)))
sdmdata <- data.frame(cbind(pb, rbind(presvals, absvals)))

```

```{r}
save(presvals, file = "presvalsDMM.RData")
load("presvalsDMM.RData")
```


Make training and testing set
```{r}

set.seed(0)
group <- kfold(canetoad, 5)
pres_train <- canetoad[group != 1, ]
pres_test <- canetoad[group == 1, ]

pres_train

```

We will look into part of the world map that is Australia. This is the areas not native for the cane toad species.
```{r}
ext <- extent(113, 153, -43, -10)

```
The first layer in the RasterStack is used as a ‘mask’. That ensures that random points only occur within the spatial extent of the rasters, and within cells that are not NA, and that there is only a single absence point per cell. Here we further restrict the background points to be within 12.5% of our specified extent ‘ext’.

The background points are divided into training and testing sets using a k-fold method (here, 5-fold). This means the data is divided into five groups; four groups are used for training (80% of the data), and one group is used for testing (20% of the data). This split is facilitated by assigning a group number to each point and then selecting points for training and testing based on these group assignments (group != 1 for training and group == 1 for testing). 

```{r}
set.seed(10)
#generate random relate point
backg <- randomPoints(predictors, n=1000, ext=ext, extf = 1.25) # (extent factor, here 1.25, meaning the extent is increased by 25%
colnames(backg) = c('lon', 'lat')

#k-fold method (here, 5-fold)
group <- kfold(backg, 5)

#splitting trainign and testing
backg_train <- backg[group != 1, ]
backg_test <- backg[group == 1, ]
```


```{r}
#create raster object from first layer of predictors
r <- raster(predictors, 1)

#raster plotting. plots the raster, but instead of showing the actual values, it displays whether each cell is NA (missing data) or not. Cells that are not NA are shown in light grey, and cells that are NA are shown in white. The legend=FALSE parameter hides the legend for this plot.
plot(!is.na(r), col=c('white', 'light grey'), legend=FALSE)

#highlight area interest in studying
plot(ext, add=TRUE, col='red', lwd=2)

#This adds the background (absence) points used for training the model to the plot. They are represented by yellow dashes (pch='-'), with a size specified by cex=0.5.
points(backg_train, pch='-', cex=0.5, col='yellow')
points(backg_test, pch='-',  cex=0.5, col='black')# background (absence) points used for testing the model.
points(pres_train, pch= '+', col='green') #presence points for training
points(pres_test, pch='+', col='blue') ##presence points for testing
```
## Profile methods
  
The three methods described here, Bioclim, Domain, and Mahal. These methods are implemented in the dismo package, and the procedures to use these models are the same for all three.

### BIOCLIM
Bioclim model using data.frame with each row representing the environmental data at known sites of presence of a species. Here we fit a bioclim model simply using the predictors and the occurrence points (the function will do the extracting for us).

```{r}
bc <- bioclim(predictors, pres_train)
?bioclim
plot(bc, a=1, b=2, p=0.85)
```
We evaluate the model in a similar way, by providing presence and background (absence) points, the model, and a RasterStack:

```{r}
e <- evaluate(pres_test, backg_test, bc, predictors)
e

#class          : ModelEvaluation 
#n presences    : 168 
#n absences     : 200 
#AUC            : 0.7201786. It means the model has a 72% chance of correctly distinguishing between presence and absence sites.
#cor            : 0.3286274. correlation between observed values and predicted values
#max TPR+TNR at : 0.02788233. The max TPR+TNR at a very low value suggests that the best cutoff for classifying presences and absences is quite low, which could be due to the data distribution or the particularities of the model.

#Look for this threshold
tr <- threshold(e, 'spec_sens')
tr
## [1] 0.02788233

```


And we use the RasterStack with predictor variables to make a prediction to a RasterLayer
(RasterStack a collection of RasterLayer objects with the same spatial extent and resolution):

```{r}
pb <- predict(predictors, bc, ext=ext, progress='')
pb
## class      : RasterLayer 
## dimensions : 792, 960, 760320  (nrow, ncol, ncell)
## resolution : 0.04166667, 0.04166667  (x, y)
## extent     : 113, 153, -43, -10  (xmin, xmax, ymin, ymax)
## crs        : +proj=longlat +datum=WGS84 +no_defs 
## source     : memory
## names      : layer 
## values     : 0, 0.5405007  (min, max)


```

```{r}
par(mfrow=c(1,2))
par(mar=c(5, 4, 4, 2) + 0.1)

plot(pb, main='Bioclim, raw values')
plot(wrld_simpl, add=TRUE, border='dark grey')
plot(pb > tr, main='presence/absence')
plot(wrld_simpl, add=TRUE, border='dark grey')
points(pres_train, pch='+')

```


### Domain
Below we fit a domain model, evaluate it, and make a prediction. We map the prediction, as well as a map subjectively classified into presence / absence.

The Domain algorithm (Carpenter et al. 1993) has been extensively used for species distribution modeling. Will explain further in the research
```{r}
dm <- domain(predictors, pres_train)
e <- evaluate(pres_test, backg_test, dm, predictors)
e
## class          : ModelEvaluation
## n presences    : 168
## n absences     : 200
## AUC            : 0.7711756 
## cor            : 0.3948721 
## max TPR+TNR at : 0.6173924 
pd = predict(predictors, dm, ext=ext, progress='')

```

```{r}
par(mfrow=c(1,2))
par(mar=c(5, 4, 4, 2) + 0.1)

plot(pd, main='Domain, raw values')
plot(wrld_simpl, add=TRUE, border='dark grey')
tr <- threshold(e, 'spec_sens')
plot(pd > tr, main='presence/absence')
plot(wrld_simpl, add=TRUE, border='dark grey')
points(pres_train, pch='+')
```
### Mahalanobis distance

```{r}
which(is.na(predictors), arr.ind=TRUE)
typeof(predictors)
colnames(predictors)
```   

```{r}
mm <- mahal(predictors, pres_train)
e <- evaluate(pres_test, backg_test, mm, predictors)
e
## class          : ModelEvaluation
## n presences    : 23
## n absences     : 200
## AUC            : 0.7686957
## cor            : 0.1506777
## max TPR+TNR at : 0.1116504
pm = predict(predictors, mm, ext=ext, progress='')
par(mfrow=c(1,2))
pm[pm < -10] <- -10
plot(pm, main='Mahalanobis distance')
plot(wrld_simpl, add=TRUE, border='dark grey')
tr <- threshold(e, 'spec_sens')
plot(pm > tr, main='presence/absence')
plot(wrld_simpl, add=TRUE, border='dark grey')
points(pres_train, pch='+')
```
## Classical regression models

With the exception of ‘maxent’, we cannot fit the model with a RasterStack and points. Instead, we need to extract the environmental data values ourselves, and fit the models with these values.

```{r}
train <- rbind(pres_train, backg_train)
pb_train <- c(rep(1, nrow(pres_train)), rep(0, nrow(backg_train)))
envtrain <- extract(predictors, train)
envtrain

extent(predictors)

envtrain <- data.frame( cbind(pa=pb_train, envtrain) )
#envtrain[,'biome'] = factor(envtrain[,'biome'], levels=1:14)
head(envtrain)
##   pa bio1 bio12 bio16 bio17 bio5 bio6 bio7 bio8 biome
## 1  1  263  1639   724    62  338  191  147  261     1
## 2  1  263  1639   724    62  338  191  147  261     1
## 3  1  253  3624  1547   373  329  150  179  271     1
## 4  1  243  1693   775   186  318  150  168  264     1
## 5  1  252  2501  1081   280  326  154  172  270     1
## 6  1  240  1214   516   146  317  150  168  261     2
testpres <- data.frame( extract(predictors, pres_test) )
testbackg <- data.frame( extract(predictors, backg_test) )

```

## Generalized Linear Models

### family = binomial(link = "logit")


```{r}
# logistic regression:
gm1 <- glm(pa ~ bclim12 +bclim15 +bclim16 +bclim2 +bclim3 +bclim5 +bclim8 +bclim6,
            family = binomial(link = "logit"), data=envtrain)

summary(gm1)

coef(gm1)

evaluate(testpres, testbackg, gm1)

```

### family = gaussian(link = "identity"), data=envtrain
```{r}
gm2 <- glm(pa ~ bclim12 +bclim15 +bclim16 +bclim2 +bclim3 +bclim5 +bclim8 +bclim6,
            family = gaussian(link = "identity"), data=envtrain)

ge2 <- evaluate(testpres, testbackg, gm2)
ge2
```

```{r}
pg <- predict(predictors, gm2, ext=ext)
par(mfrow=c(1,2))
plot(pg, main='GLM/gaussian, raw values')
plot(wrld_simpl, add=TRUE, border='dark grey')
tr <- threshold(ge2, 'spec_sens')
plot(pg > tr, main='presence/absence')
plot(wrld_simpl, add=TRUE, border='dark grey')
points(pres_train, pch='+')
points(backg_train, pch='-', cex=0.25)

```

### Generalized Additive Models

## Machine learning methods

Methods include Artifical Neural Networks (ANN), Random Forests, Boosted Regression Trees, and Support Vector Machines.

MaxEnt (short for “Maximum Entropy”; Phillips et al., 2006) is the most widely used SDM algorithm.

```{r}
maxent()
## Loading required namespace: rJava. So download Java 64 bit for windows
## This is MaxEnt version 3.4.3
xm <- maxent(predictors, pres_train)
## This is MaxEnt version 3.4.3
plot(xm)
```
A response plot:
```{r}
par("mar")
par(mar=c(1,1,1,1))

response(xm)

```

```{r}
e <- evaluate(pres_test, backg_test, xm, predictors)
e
## class          : ModelEvaluation
## n presences    : 23
## n absences     : 200
## AUC            : 0.8336957
## cor            : 0.3789954
## max TPR+TNR at : 0.1772358
px <- predict(predictors, xm, ext=ext, progress='')
par(mfrow=c(1,2))
plot(px, main='Maxent, raw values')
plot(wrld_simpl, add=TRUE, border='dark grey')
tr <- threshold(e, 'spec_sens')
plot(px > tr, main='presence/absence')
plot(wrld_simpl, add=TRUE, border='dark grey')
points(pres_train, pch='+')
```
### Boosted Regression Trees


### Random Forest
 rf1 does regression, rf2 and rf3 do classification (they are exactly the same models). See the function tuneRF for optimizing the model fitting procedure.
 
```{r}
library(randomForest)


model <- pa ~ bclim12 +bclim15 +bclim16 +bclim2 +bclim3 +bclim5 +bclim8 +bclim6
rf1 <- randomForest(model, data=envtrain)

model <- factor(pa) ~ bclim12 +bclim15 +bclim16 +bclim2 +bclim3 +bclim5 +bclim8 +bclim6
rf2 <- randomForest(model, data=envtrain)
rf3 <- randomForest(envtrain[,1:8], factor(pb_train))
erf <- evaluate(testpres, testbackg, rf1)
erf
```

```{r}
pr <- predict(predictors, rf1, ext=ext)
par(mfrow=c(1,2))
plot(pr, main='Random Forest, regression')
plot(wrld_simpl, add=TRUE, border='dark grey')
tr <- threshold(erf, 'spec_sens')
plot(pr > tr, main='presence/absence')
plot(wrld_simpl, add=TRUE, border='dark grey')
points(pres_train, pch='+')
points(backg_train, pch='-', cex=0.25)

```
### Support Vector Machines

unction ‘ksvm’ in package ‘kernlab’ and the ‘svm’ function in package ‘e1071’. ‘ksvm’ includes many different SVM formulations and kernels and provides useful options and features like a method for plotting, but it lacks a proper model selection tool. The ‘svm’ function in package ‘e1071’ includes a model selection tool: the ‘tune’ function (Karatzoglou et al., 2006)
```{r}
library(kernlab)
##
## Attaching package: 'kernlab'
## The following objects are masked from 'package:raster':
##
##     buffer, rotated
svm <- ksvm(pa ~ bclim12 +bclim15 +bclim16 +bclim2 +bclim3 +bclim5 +bclim8 +bclim6, data=envtrain)

#svm <- ksvm(pa ~ ., data=envtrain)
esv <- evaluate(testpres, testbackg, svm)
esv
## class          : ModelEvaluation 
## n presences    : 168 
## n absences     : 200 
## AUC            : 0.907381 
## cor            : 0.6926611 
## max TPR+TNR at : 0.320362
ps <- predict(predictors, svm, ext=ext)
par(mfrow=c(1,2))
plot(ps, main='Support Vector Machine')
plot(wrld_simpl, add=TRUE, border='dark grey')
tr <- threshold(esv, 'spec_sens')
plot(ps > tr, main='presence/absence')
plot(wrld_simpl, add=TRUE, border='dark grey')
points(pres_train, pch='+')
points(backg_train, pch='-', cex=0.25)
```

### Combining model predictions
```{r}
models <- stack(pb, pd, pg, pr, ps)
names(models) <- c("BIOCLIM", "Domain algorithm", "GLM", "Random Forest", "Support Vector Machine")
plot(models)
```

Now we can compute the simple average:

```{r}
m <- mean(models)
plot(m, main='Average Score of 5 different models')
```

However, this is a problematic approach as the values predicted by the models are not all on the same (between 0 and 1) scale; so you may want to fix that first. Another concern could be weighting. Let’s combine three models weighted by their AUC scores. Here, to create the weights, we substract 0.5 (the random expectation) and square the result to give further weight to higher AUC values.

```{r}

library(caret)


# Make predictions on the test dataset
#predictions <- predict(predictors, svm, ext=ext)

# Generate the confusion matrix
confusionMatrix <- confusionMatrix(predictions, envtest$pa)

# Print the confusion matrix
print(confusionMatrix)

# Calculate performance metrics
accuracy <- confusionMatrix$overall['Accuracy']
precision <- confusionMatrix$byClass['Precision']
recall <- confusionMatrix$byClass['Recall']
specificity <- confusionMatrix$byClass['Specificity']
F1_score <- 2 * (precision * recall) / (precision + recall)
AUC <- roc(envtest$pa, as.numeric(predictions))$auc

# Print performance metrics
cat("Accuracy:", accuracy, "\n")
cat("Precision:", precision, "\n")
cat("Recall (Sensitivity):", recall, "\n")
cat("Specificity:", specificity, "\n")
cat("F1 Score:", F1_score, "\n")
cat("AUC:", AUC, "\n")

```